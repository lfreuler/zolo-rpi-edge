# ZoLo Part 3: Container-Based NAS
*Samba Container auf Docker Swarm mit USB Storage*

---

## √úbersicht

Part 3 erweitert dein bestehendes GlusterFS Setup um eine einfache, Container-basierte NAS-L√∂sung f√ºr Family File Sharing.

### Was du bekommst

```yaml
Storage Tiers:
  Tier 1 (GlusterFS/NVMe): 
    - Docker Services
    - Admin Scripts & Configs
    - Web Content
    
  Tier 2 (USB NAS): 
    - Family File Sharing
    - Personal Folders
    - Media & Documents

Access:
  - Windows: \\192.168.1.122\share
  - Linux: mount -t cifs //192.168.1.122/share
  - Multi-User mit Permissions

Tech Stack:
  ‚úÖ Docker Swarm (bereits vorhanden)
  ‚úÖ dperson/samba Container
  ‚úÖ keepalived VIP (bereits vorhanden)
  ‚úÖ Auto-Failover capable
```

---

## Hardware Erg√§nzung

```yaml
Zus√§tzlich zu bestehendem Setup:

USB Storage (Empfehlung):
  - 2x USB 3.0/3.1 SSD (1-2TB)
  - Samsung T7, SanDisk Extreme, oder √§hnlich
  - An zolo-pi-3 und zolo-pi-4
  
Warum USB statt GlusterFS?
  ‚úÖ Windows-lesbar im Notfall (ext4 + Tool)
  ‚úÖ Einfaches Backup (USB abstecken)
  ‚úÖ Transparente Dateien
  ‚úÖ Weniger Overhead f√ºr gro√üe Files
```

---

## Storage Architektur

### Gesamt√ºbersicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Storage Tier 1: NVMe GlusterFS (alle 4 Nodes)     ‚îÇ
‚îÇ /mnt/glusterfs-swarm/ (~760GB usable)             ‚îÇ
‚îÇ ‚îú‚îÄ scripts/      # Docker Configs, Admin Tools    ‚îÇ
‚îÇ ‚îî‚îÄ webapp/       # Web Content, Static Files      ‚îÇ
‚îÇ Via: NFS (192.168.1.121) oder direkt gemountet    ‚îÇ
‚îÇ Use: Docker Volumes, Fast Storage                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Storage Tier 2: USB NAS (pi-3, pi-4)              ‚îÇ
‚îÇ /mnt/usb-nas-master/data/ (~1-2TB)                 ‚îÇ
‚îÇ ‚îú‚îÄ shared/       # Family Sharing                 ‚îÇ
‚îÇ ‚îú‚îÄ luggie/       # Personal (luggie + nasadm)     ‚îÇ
‚îÇ ‚îú‚îÄ emi/          # Personal (emi + nasadm)        ‚îÇ
‚îÇ ‚îú‚îÄ lele/         # Personal (lele + nasadm)       ‚îÇ
‚îÇ ‚îî‚îÄ riri/         # Personal (riri + nasadm)       ‚îÇ
‚îÇ Via: Samba Container (192.168.1.122)              ‚îÇ
‚îÇ Use: Family Files, Media, Documents                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Samba Container Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Docker Swarm (Orchestration)             ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ ‚îÇ Samba Container            ‚îÇ          ‚îÇ
‚îÇ ‚îÇ (dperson/samba)            ‚îÇ          ‚îÇ
‚îÇ ‚îÇ Port 445, 139              ‚îÇ          ‚îÇ
‚îÇ ‚îÇ                            ‚îÇ          ‚îÇ
‚îÇ ‚îÇ Shares:                    ‚îÇ          ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ shared ‚Üí /usb/shared     ‚îÇ          ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ luggie ‚Üí /usb/luggie     ‚îÇ          ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ storage ‚Üí /gluster       ‚îÇ          ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ          ‚îÇ                               ‚îÇ
‚îÇ          ‚ñº                               ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ ‚îÇ Host Volumes               ‚îÇ          ‚îÇ
‚îÇ ‚îÇ /mnt/usb-nas-master/data   ‚îÇ          ‚îÇ
‚îÇ ‚îÇ /mnt/glusterfs-swarm       ‚îÇ          ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
  Client Access via VIP 192.168.1.122
  \\192.168.1.122\shared
```

---

## Installation

### Phase 1: Host Services Cleanup

```bash
#!/bin/bash
# 00-cleanup-host-services.sh
# Auf pi-3 UND pi-4 ausf√ºhren

set -e

echo "=== Cleanup Host Services ==="
echo "Node: $(hostname)"
echo ""

# Stoppe alte Samba Services
echo "1. Stoppe Samba Services..."
sudo systemctl stop smbd nmbd 2>/dev/null || true
sudo systemctl disable smbd nmbd 2>/dev/null || true

# Samba Config sichern
if [ -f /etc/samba/smb.conf ]; then
    sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.backup
fi

# Port Check
echo ""
echo "2. Port Check..."
if sudo netstat -tlnp 2>/dev/null | grep -E ':(445|139) '; then
    echo "‚ö†Ô∏è  Ports noch belegt - bitte manuell pr√ºfen!"
else
    echo "‚úÖ Ports 445, 139 frei"
fi

# keepalived bleibt aktiv!
echo ""
echo "3. keepalived Status (soll laufen)..."
systemctl is-active keepalived && echo "‚úÖ keepalived aktiv"

echo ""
echo "‚úÖ Cleanup abgeschlossen!"
```

### Phase 2: USB Setup

```bash
#!/bin/bash
# 01-setup-usb-storage.sh
# Auf pi-3 UND pi-4 ausf√ºhren

set -e

NODE=$(hostname)
echo "=== USB Storage Setup auf $NODE ==="
echo ""

# USB Device (anpassen falls n√∂tig)
USB_DEVICE="/dev/sda"

# Mount Points
if [ "$NODE" == "zolo-pi-3" ]; then
    MOUNT_POINT="/mnt/usb-nas-master"
elif [ "$NODE" == "zolo-pi-4" ]; then
    MOUNT_POINT="/mnt/usb-nas-backup"
else
    echo "‚ùå Nur auf pi-3 oder pi-4 ausf√ºhren!"
    exit 1
fi

echo "USB Device: $USB_DEVICE"
echo "Mount Point: $MOUNT_POINT"
echo ""

# 1. Partition erstellen (ext4)
echo "1. Erstelle Partition (ext4)..."
echo "‚ö†Ô∏è  Alle Daten auf $USB_DEVICE werden gel√∂scht!"
read -p "Fortfahren? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    exit 0
fi

sudo parted $USB_DEVICE --script mklabel gpt
sudo parted $USB_DEVICE --script mkpart primary ext4 0% 100%
sleep 2

# 2. Filesystem erstellen
USB_PARTITION="${USB_DEVICE}1"
echo ""
echo "2. Erstelle ext4 Filesystem..."
sudo mkfs.ext4 -F -L "usb-nas-${NODE##*-}" $USB_PARTITION

# 3. Mount Point erstellen
echo ""
echo "3. Erstelle Mount Point..."
sudo mkdir -p $MOUNT_POINT

# 4. Permanent Mount (fstab)
USB_UUID=$(sudo blkid -s UUID -o value $USB_PARTITION)
echo ""
echo "4. F√ºge zu /etc/fstab hinzu..."
echo "UUID=$USB_UUID $MOUNT_POINT ext4 defaults,nofail 0 2" | \
    sudo tee -a /etc/fstab

# 5. Mount
sudo mount $MOUNT_POINT

# 6. Struktur erstellen (nur auf pi-3)
if [ "$NODE" == "zolo-pi-3" ]; then
    echo ""
    echo "6. Erstelle Verzeichnisstruktur..."
    sudo mkdir -p $MOUNT_POINT/data/{shared,luggie,emi,lele,riri}
    
    # Permissions f√ºr Container
    sudo chown -R pi:pi $MOUNT_POINT/data
    sudo chmod 777 $MOUNT_POINT/data
    sudo chmod 777 $MOUNT_POINT/data/{shared,luggie,emi,lele,riri}
    
    # README
    cat << 'EOF' | sudo tee $MOUNT_POINT/data/README.txt
ZoLo NAS Storage
================

Shares:
  shared/  - Gemeinsam f√ºr alle
  luggie/  - Pers√∂nlich (luggie + nasadm)
  emi/     - Pers√∂nlich (emi + nasadm)
  lele/    - Pers√∂nlich (lele + nasadm)
  riri/    - Pers√∂nlich (riri + nasadm)

Access: \\192.168.1.122\<share>
EOF
fi

# 7. Symlink erstellen (auf pi-4 f√ºr einheitliche Pfade)
if [ "$NODE" == "zolo-pi-4" ]; then
    echo ""
    echo "7. Erstelle Symlink f√ºr Docker Volumes..."
    sudo ln -sf /mnt/usb-nas-backup /mnt/usb-nas-master
    echo "‚úÖ Symlink erstellt: /mnt/usb-nas-master -> /mnt/usb-nas-backup"
fi

# 8. Status
echo ""
echo "=== Setup abgeschlossen ==="
df -h $MOUNT_POINT
ls -la $MOUNT_POINT/

# Verify Symlink (pi-4)
if [ "$NODE" == "zolo-pi-4" ]; then
    echo ""
    echo "Symlink Check:"
    ls -la /mnt/ | grep usb-nas
fi

echo ""
echo "‚úÖ USB Storage bereit!"
```

### Phase 3: GlusterFS Permissions & Symlinks

```bash
#!/bin/bash
# 02-setup-glusterfs-permissions.sh
# Auf pi-3 UND pi-4 ausf√ºhren

NODE=$(hostname)
echo "=== GlusterFS Permissions & Symlinks f√ºr Samba ==="
echo "Node: $NODE"
echo ""

# Permissions f√ºr Container
sudo chmod 777 /mnt/glusterfs-swarm

# Struktur erstellen (nur auf pi-3)
if [ "$NODE" == "zolo-pi-3" ]; then
    echo "Erstelle Struktur auf pi-3..."
    sudo mkdir -p /mnt/glusterfs-swarm/{scripts,webapp}
    sudo chmod -R 777 /mnt/glusterfs-swarm/scripts
    sudo chmod -R 777 /mnt/glusterfs-swarm/webapp
fi

# Symlink f√ºr Failover (auf pi-4)
if [ "$NODE" == "zolo-pi-4" ]; then
    echo "Erstelle Symlink f√ºr Docker Volume Consistency..."
    sudo ln -sf /mnt/usb-nas-backup /mnt/usb-nas-master
    
    echo ""
    echo "Symlink Check:"
    ls -la /mnt/ | grep usb-nas
    echo ""
    echo "‚ÑπÔ∏è  Docker Stack nutzt /mnt/usb-nas-master auf allen Nodes"
    echo "   pi-3: real path"
    echo "   pi-4: symlink -> /mnt/usb-nas-backup"
fi

echo ""
echo "‚úÖ Setup abgeschlossen auf $NODE"
```

**Warum der Symlink wichtig ist:**

```yaml
Problem ohne Symlink:
  pi-3 gestoppt ‚Üí Swarm startet Container auf pi-4
  Stack sucht: /mnt/usb-nas-master
  pi-4 hat: /mnt/usb-nas-backup
  Result: Mount Error ‚ùå

L√∂sung mit Symlink:
  pi-4: /mnt/usb-nas-master -> /mnt/usb-nas-backup
  Stack findet: /mnt/usb-nas-master ‚úÖ
  Container startet erfolgreich ‚úÖ
  Failover funktioniert! ‚úÖ
```

### Phase 4: Docker Stack Deployment

**Wichtig:** Erst deployen NACH USB Setup auf beiden Nodes!

```yaml
# samba-nas-stack.yml
# Auf pi-3 speichern

version: '3.8'

services:
  samba:
    image: dperson/samba:latest
    ports:
      - target: 445
        published: 445
        mode: host
      - target: 139
        published: 139
        mode: host
    environment:
      # Users: username;password
      - USER=luggie;LuggieSecure123
      - USER2=emi;EmiSecure123
      - USER3=lele;LeleSecure123
      - USER4=riri;RiriSecure123
      - USER5=nasadm;AdminSecure123
      
      # USB Shares
      - SHARE=shared;/usb/shared;yes;no;no;luggie,emi,lele,riri,nasadm;nasadm;0777;0777
      - SHARE2=luggie;/usb/luggie;yes;no;no;luggie,nasadm;nasadm;0777;0777
      - SHARE3=emi;/usb/emi;yes;no;no;emi,nasadm;nasadm;0777;0777
      - SHARE4=lele;/usb/lele;yes;no;no;lele,nasadm;nasadm;0777;0777
      - SHARE5=riri;/usb/riri;yes;no;no;riri,nasadm;nasadm;0777;0777
      
      # GlusterFS Share (Admin only)
      - SHARE6=storage;/gluster;yes;no;no;nasadm;nasadm;0777;0777
      
      - WORKGROUP=WORKGROUP
      
    volumes:
      - /mnt/usb-nas-master/data:/usb:rw
      - /mnt/glusterfs-swarm:/gluster:rw
      
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.hostname == zolo-pi-3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    networks:
      - nas

networks:
  nas:
    driver: overlay
```

```bash
# Deploy
docker stack deploy -c samba-nas-stack.yml nas

# Status
docker service ls
docker service ps nas_samba

# Logs
docker service logs nas_samba -f
```

### Phase 5: Auto-Sync Setup

```bash
# 03-install-nas-sync.sh
# Auf pi-3 UND pi-4 ausf√ºhren
# (Siehe Artifact "03-install-nas-sync.sh" weiter oben)

# Was es macht:
# - SSH Key Setup zwischen Nodes
# - Sync Script Installation
# - Cron Job (st√ºndlich um :20)
# - Status Script
# - Initial Test Sync

# Deployment:
bash 03-install-nas-sync.sh

# Auf beiden Nodes ausf√ºhren!
# SSH Passwort wird einmal abgefragt (f√ºr Key Exchange)
```

### Phase 6: keepalived mit nopreempt

```bash
# Auf pi-3:
bash 05-keepalived-config-pi3.sh

# Auf pi-4:
bash 05-keepalived-config-pi4.sh

# Verify:
ip addr show eth0 | grep 192.168.1.122

# Test (siehe "Failover Test Scenarios" oben)
```

---

## Client Setup

### Windows Client

```powershell
# Explorer: Win + R
\\192.168.1.122

# Drive Maps erstellen
net use L: \\192.168.1.122\luggie /user:luggie LuggieSecure123
net use S: \\192.168.1.122\shared /user:luggie LuggieSecure123
net use E: \\192.168.1.122\emi /user:emi EmiSecure123

# Admin (GlusterFS)
net use A: \\192.168.1.122\storage /user:nasadm AdminSecure123

# Persistent speichern
net use L: \\192.168.1.122\luggie /user:luggie LuggieSecure123 /persistent:yes
```

### Linux/macOS Client

```bash
# Installation
sudo apt install cifs-utils  # Ubuntu/Debian
brew install samba           # macOS

# Mount
sudo mkdir -p /mnt/nas
sudo mount -t cifs //192.168.1.122/shared /mnt/nas \
  -o username=luggie,password=LuggieSecure123,vers=3.0

# Persistent (fstab)
echo "//192.168.1.122/shared /mnt/nas cifs username=luggie,password=LuggieSecure123,vers=3.0,_netdev 0 0" | \
  sudo tee -a /etc/fstab
```

### macOS Finder

```
# Finder ‚Üí Go ‚Üí Connect to Server (‚åòK)
smb://192.168.1.122

# Oder spezifisch:
smb://luggie@192.168.1.122/luggie
```

---

## Management & Monitoring

### Service Status Check

```bash
#!/bin/bash
# check-nas-status.sh

echo "=== ZoLo NAS Status ==="
echo ""

# Docker Service
echo "Docker Service:"
docker service ls | grep nas_samba
echo ""

# Welcher Node?
echo "Running on:"
docker service ps nas_samba --filter "desired-state=running" \
  --format "table {{.Name}}\t{{.Node}}\t{{.CurrentState}}"
echo ""

# Storage
echo "Storage Usage:"
df -h /mnt/usb-nas-master | tail -1
df -h /mnt/glusterfs-swarm | tail -1
echo ""

# VIP Status
echo "VIP Status (192.168.1.122):"
if ip addr show eth0 | grep -q "192.168.1.122"; then
    echo "‚úÖ VIP aktiv auf diesem Node"
else
    echo "‚ÑπÔ∏è  VIP auf anderem Node"
fi
echo ""

# Ports
echo "Samba Ports:"
sudo netstat -tlnp 2>/dev/null | grep -E ':(445|139) ' || echo "Keine Ports belegt"
```

### Quick Commands

```bash
# Service Status
docker service ls
docker service ps nas_samba

# Logs (live)
docker service logs nas_samba -f

# Logs (last 50 lines)
docker service logs nas_samba --tail 50

# Container restart
docker service update --force nas_samba

# Stack neu deployen
docker stack deploy -c samba-nas-stack.yml nas

# Stack entfernen
docker stack rm nas

# In Container reingehen (debug)
docker exec -it $(docker ps -q -f name=nas_samba) bash
```

---

## Troubleshooting

### Container startet nicht

```bash
# Fehler anzeigen
docker service ps nas_samba --no-trunc

# H√§ufige Probleme:

# 1. Ports belegt
sudo netstat -tlnp | grep -E ':(445|139) '
sudo systemctl stop smbd nmbd

# 2. Volume nicht gemountet
mountpoint /mnt/usb-nas-master
mountpoint /mnt/glusterfs-swarm

# 3. Permissions
ls -la /mnt/usb-nas-master/data/
sudo chmod -R 777 /mnt/usb-nas-master/data

# 4. Mount Error auf pi-4 (Symlink fehlt)
# Error: "invalid mount config for type bind"
# Auf pi-4:
sudo ln -sf /mnt/usb-nas-backup /mnt/usb-nas-master
ls -la /mnt/ | grep usb-nas
docker service update --force nas_samba
```

### Kann nicht schreiben

```bash
# Permissions pr√ºfen
ls -la /mnt/usb-nas-master/data/

# Fix
sudo chmod 777 /mnt/usb-nas-master/data/{shared,luggie,emi,lele,riri}
sudo chmod -R 777 /mnt/glusterfs-swarm

# Container restart
docker service update --force nas_samba
```

### Shares nicht sichtbar

```bash
# Check Samba Config im Container
docker exec $(docker ps -q -f name=nas_samba) cat /etc/samba/smb.conf

# User Check
docker exec $(docker ps -q -f name=nas_samba) pdbedit -L

# Test von Host
smbclient -L localhost -U luggie%LuggieSecure123
```

### VIP nicht erreichbar

```bash
# VIP Status
ip addr show eth0 | grep 192.168.1.122

# keepalived Check
sudo systemctl status keepalived

# Logs
sudo journalctl -u keepalived -f
```

---

## Backup Strategy (Optional)

### Manuelle Backups

```bash
# USB -> External HDD
sudo rsync -avz --progress \
  /mnt/usb-nas-master/data/ \
  /mnt/external-backup/

# GlusterFS -> External HDD
sudo rsync -avz --progress \
  /mnt/glusterfs-swarm/ \
  /mnt/external-backup/gluster/
```

### Automatisches Sync pi-3 ‚Üí pi-4

```bash
# Sp√§ter implementierbar mit:
# - rsync Container (scheduled)
# - lsyncd (realtime)
# - Unison (bidirectional)

# F√ºr jetzt: Manuell wenn n√∂tig
rsync -avz /mnt/usb-nas-master/data/ \
  pi@192.168.1.104:/mnt/usb-nas-backup/data/
```

---

## High Availability (Erweiterung)

### Failover Strategie mit nopreempt

**Problem ohne nopreempt:**
```yaml
pi-3 down ‚Üí VIP zu pi-4 ‚Üí Container zu pi-4
pi-3 up   ‚Üí VIP zur√ºck zu pi-3 ‚Üí Container zur√ºck zu pi-3
Problem: Unn√∂tige Switches, kurze Downtimes
```

**L√∂sung mit nopreempt:**
```yaml
Regel: VIP bleibt wo sie ist (au√üer bei Node-Ausfall)

Normale Situation:
  pi-3 bootet zuerst ‚Üí bekommt VIP ‚Üí Container auf pi-3 ‚úÖ

Failover (pi-3 down):
  pi-4 √ºbernimmt VIP ‚Üí Container relocated zu pi-4 ‚úÖ

Recovery (pi-3 up):
  nopreempt ‚Üí VIP BLEIBT auf pi-4 ‚úÖ
  ‚Üí Container BLEIBT auf pi-4 ‚úÖ
  ‚Üí Stable, keine unn√∂tigen Switches!

Failback (pi-4 down):
  ‚Üí VIP zur√ºck zu pi-3 (einziger Node) ‚úÖ
  ‚Üí Container zur√ºck zu pi-3 ‚úÖ
  ‚Üí Automatic Failback bei totalem Ausfall!
```

### keepalived Config - pi-3 (mit nopreempt)

```bash
#!/bin/bash
# 05-keepalived-config-pi3.sh
# Auf zolo-pi-3 ausf√ºhren

sudo tee /etc/keepalived/keepalived.conf << 'EOF'
global_defs {
    router_id ZOLO_PI3
    enable_script_security
    script_user root
}

vrrp_script check_nfs {
    script "/usr/bin/systemctl is-active nfs-kernel-server"
    interval 2
    timeout 3
    weight -20
    fall 2
    rise 2
}

vrrp_script check_gluster_mount {
    script "/bin/mountpoint -q /mnt/glusterfs-swarm"
    interval 3
    timeout 3
    weight -20
    fall 2
    rise 2
}

# NFS/GlusterFS VIP (Standard Failover)
vrrp_instance VI_SWARM {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 80
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass zolo_swarm_2024
    }
    
    virtual_ipaddress {
        192.168.1.121/24 dev eth0
    }
    
    track_script {
        check_nfs
        check_gluster_mount
    }
}

# Samba VIP (mit nopreempt f√ºr Stabilit√§t)
vrrp_instance VI_SAMBA {
    state BACKUP           # Nicht MASTER! Wichtig f√ºr nopreempt
    nopreempt             # Kein automatisches Zur√ºckholen der VIP
    interface eth0
    virtual_router_id 52
    priority 100          # H√∂chste Priority (f√ºr initiales Setup)
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass zolo_samba_2024
    }
    
    virtual_ipaddress {
        192.168.1.122/24 dev eth0
    }
}
EOF

sudo systemctl restart keepalived
echo "‚úÖ keepalived configured on pi-3 with nopreempt"
```

### keepalived Config - pi-4 (mit nopreempt)

```bash
#!/bin/bash
# 05-keepalived-config-pi4.sh
# Auf zolo-pi-4 ausf√ºhren

sudo tee /etc/keepalived/keepalived.conf << 'EOF'
global_defs {
    router_id ZOLO_PI4
    enable_script_security
    script_user root
}

vrrp_script check_nfs {
    script "/usr/bin/systemctl is-active nfs-kernel-server"
    interval 2
    timeout 3
    weight -20
    fall 2
    rise 2
}

vrrp_script check_gluster_mount {
    script "/bin/mountpoint -q /mnt/glusterfs-swarm"
    interval 3
    timeout 3
    weight -20
    fall 2
    rise 2
}

# NFS/GlusterFS VIP
vrrp_instance VI_SWARM {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 70
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass zolo_swarm_2024
    }
    
    virtual_ipaddress {
        192.168.1.121/24 dev eth0
    }
    
    track_script {
        check_nfs
        check_gluster_mount
    }
}

# Samba VIP (mit nopreempt)
vrrp_instance VI_SAMBA {
    state BACKUP
    nopreempt             # Auch hier nopreempt
    interface eth0
    virtual_router_id 52
    priority 90           # Niedriger als pi-3
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass zolo_samba_2024
    }
    
    virtual_ipaddress {
        192.168.1.122/24 dev eth0
    }
}
EOF

sudo systemctl restart keepalived
echo "‚úÖ keepalived configured on pi-4 with nopreempt"
```

### Failover Test Scenarios

```bash
#!/bin/bash
# test-failover.sh
# Testet alle Failover Szenarien

echo "=== NAS Failover Tests ==="
echo ""

# Test 1: Normal Boot
echo "Test 1: Beide Nodes booten gleichzeitig"
echo "  Expected: pi-3 bekommt VIP (priority 100 > 90)"
echo "  Check: ip addr show eth0 | grep 192.168.1.122"
echo ""

# Test 2: pi-3 Failover
echo "Test 2: pi-3 f√§llt aus"
echo "  Run on pi-3: sudo systemctl stop keepalived"
echo "  Expected: VIP wechselt zu pi-4"
echo "  Expected: Container relocated zu pi-4"
echo "  Check: docker service ps nas_samba"
echo ""

# Test 3: pi-3 Recovery (nopreempt)
echo "Test 3: pi-3 kommt zur√ºck"
echo "  Run on pi-3: sudo systemctl start keepalived"
echo "  Expected: VIP BLEIBT auf pi-4 (nopreempt!)"
echo "  Expected: Container BLEIBT auf pi-4"
echo "  Check: VIP und Container auf gleichem Node?"
echo ""

# Test 4: pi-4 Failback
echo "Test 4: pi-4 f√§llt aus (w√§hrend pi-4 VIP hat)"
echo "  Run on pi-4: sudo systemctl stop keepalived"
echo "  Expected: VIP zur√ºck zu pi-3 (einziger Node)"
echo "  Expected: Container relocated zu pi-3"
echo "  Check: Service verf√ºgbar √ºber VIP?"
echo ""

# Test 5: pi-4 Recovery
echo "Test 5: pi-4 kommt zur√ºck"
echo "  Run on pi-4: sudo systemctl start keepalived"
echo "  Expected: VIP BLEIBT auf pi-3 (nopreempt!)"
echo "  Expected: Stable, keine Switches"
echo ""

# Quick Status Check
check_status() {
    echo "Current Status:"
    echo "  VIP auf: $(ssh pi@192.168.1.103 "ip addr show eth0 | grep -q 192.168.1.122 && echo pi-3" || echo pi-4)"
    echo "  Container auf: $(docker service ps nas_samba --filter "desired-state=running" --format "{{.Node}}" | head -1)"
}

check_status
```

### VIP & Container Status Check

```bash
#!/bin/bash
# check-vip-container.sh
# Zeigt VIP und Container Status

echo "=== VIP & Container Status ==="
echo ""
echo "Node: $(hostname)"
echo ""

# VIP Status
echo "VIP 192.168.1.122:"
if ip addr show eth0 | grep -q "192.168.1.122"; then
    echo "  üü¢ Aktiv auf diesem Node"
else
    echo "  ‚ö™ Nicht auf diesem Node"
    
    # Check auf anderem Node
    if [ "$(hostname)" == "zolo-pi-3" ]; then
        OTHER="192.168.1.104"
    else
        OTHER="192.168.1.103"
    fi
    
    if ping -c 1 -W 1 $OTHER &>/dev/null; then
        if ssh pi@$OTHER "ip addr show eth0 | grep -q 192.168.1.122" 2>/dev/null; then
            echo "  ‚Üí VIP ist auf anderem Node"
        fi
    fi
fi

echo ""

# Container Status
echo "Samba Container:"
CONTAINER_NODE=$(docker service ps nas_samba --filter "desired-state=running" --format "{{.Node}}" 2>/dev/null | head -1)
if [ -n "$CONTAINER_NODE" ]; then
    echo "  üì¶ L√§uft auf: $CONTAINER_NODE"
    
    if [ "$CONTAINER_NODE" == "$(hostname)" ]; then
        echo "  üü¢ Container ist LOKAL"
    else
        echo "  ‚ö™ Container ist REMOTE"
    fi
else
    echo "  ‚ùå Container l√§uft nicht!"
fi

echo ""

# Sync Direction
echo "Sync Status:"
if ip addr show eth0 | grep -q "192.168.1.122"; then
    echo "  ‚Üí Dieser Node synct zu BACKUP"
    echo "     (Master synct st√ºndlich)"
else
    echo "  ‚Üê Dieser Node empf√§ngt Sync"
    echo "     (Backup empf√§ngt vom Master)"
fi

echo ""

# Summary
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
HAS_VIP=$(ip addr show eth0 | grep -q "192.168.1.122" && echo "yes" || echo "no")
HAS_CONTAINER=$([ "$CONTAINER_NODE" == "$(hostname)" ] && echo "yes" || echo "no")

if [ "$HAS_VIP" == "yes" ] && [ "$HAS_CONTAINER" == "yes" ]; then
    echo "‚úÖ PERFEKT: VIP und Container sind zusammen!"
    echo "   Dieser Node ist ACTIVE MASTER"
elif [ "$HAS_VIP" == "no" ] && [ "$HAS_CONTAINER" == "no" ]; then
    echo "‚úÖ OK: Backup Node (kein VIP, kein Container)"
    echo "   Dieser Node ist PASSIVE BACKUP"
else
    echo "‚ö†Ô∏è  MISMATCH: VIP und Container nicht zusammen!"
    echo "   Das ist normal direkt nach Failover"
    echo "   Swarm relocated Container automatisch in ~30s"
fi

echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
```

### Warum nopreempt?

```yaml
Vorteile von nopreempt:
  ‚úÖ Stabilit√§t: Keine unn√∂tigen VIP-Wechsel
  ‚úÖ Weniger Downtime: Kein Failback bei Recovery
  ‚úÖ Sync-Safe: VIP und Container bleiben zusammen
  ‚úÖ Simple: Keine komplexen Scripts n√∂tig
  ‚úÖ Automatic Failback: Bei totalem Ausfall wechselt zur√ºck

Verhalten:
  Normal ‚Üí VIP auf pi-3 ‚Üí Container auf pi-3
  pi-3 down ‚Üí VIP zu pi-4 ‚Üí Container zu pi-4
  pi-3 up ‚Üí VIP BLEIBT pi-4 ‚Üí Container BLEIBT pi-4 (stable!)
  pi-4 down ‚Üí VIP zu pi-3 ‚Üí Container zu pi-3 (failback!)
  
Manuelles Failback (wenn gew√ºnscht):
  1. Stop keepalived auf current master
  2. VIP wechselt automatisch
  3. Container relocated automatisch
  4. Start keepalived wieder
```

---

## Performance Tuning

### Samba Tuning (falls n√∂tig)

Erweitere Environment Variables im Stack:

```yaml
environment:
  # ... existing vars ...
  - GLOBAL=socket options = TCP_NODELAY IPTOS_LOWDELAY SO_RCVBUF=131072 SO_SNDBUF=131072
  - GLOBAL2=read raw = yes
  - GLOBAL3=write raw = yes
  - GLOBAL4=min receivefile size = 16384
  - GLOBAL5=use sendfile = true
  - GLOBAL6=aio read size = 16384
  - GLOBAL7=aio write size = 16384
```

### USB Performance Check

```bash
# Write Test
sudo dd if=/dev/zero of=/mnt/usb-nas-master/test bs=1M count=1000 oflag=direct

# Read Test
sudo dd if=/mnt/usb-nas-master/test of=/dev/null bs=1M

# Cleanup
sudo rm /mnt/usb-nas-master/test

# Erwartet: 100-400 MB/s (USB 3.0/3.1 SSD)
```

---

## Deployment Checklist

```
Pre-Deployment:
‚ñ° Docker Swarm l√§uft
‚ñ° GlusterFS Volume mounted
‚ñ° keepalived VIP configured
‚ñ° Alte Samba Services gestoppt

USB Setup (beide Nodes):
‚ñ° USB SSD angeschlossen
‚ñ° Partition erstellt (ext4)
‚ñ° Gemountet unter /mnt/usb-nas-*
‚ñ° Permissions gesetzt (777)
‚ñ° Struktur erstellt (pi-3)
‚ñ° Symlink erstellt (pi-4): /mnt/usb-nas-master -> /mnt/usb-nas-backup

Container Deployment:
‚ñ° Stack File erstellt
‚ñ° Passw√∂rter angepasst
‚ñ° Stack deployed
‚ñ° Service l√§uft
‚ñ° Logs OK

Auto-Sync Setup:
‚ñ° Sync Script installiert (beide Nodes)
‚ñ° SSH Keys ausgetauscht
‚ñ° Cron Job aktiv
‚ñ° Initial Sync erfolgreich
‚ñ° Status Script funktioniert

Client Tests:
‚ñ° Windows Client verbindet
‚ñ° Shares sichtbar
‚ñ° Dateien erstellen funktioniert
‚ñ° User Permissions korrekt
‚ñ° Admin kann auf storage zugreifen

High Availability (optional):
‚ñ° keepalived mit nopreempt konfiguriert (beide Nodes)
‚ñ° VIP Failover getestet
‚ñ° Container Relocation getestet
‚ñ° Sync Direction verifiziert
‚ñ° Status Check Script installiert
```

---

## Summary

### Was du jetzt hast

```
ZoLo Container NAS Setup

Komponenten:
  ‚Ä¢ Samba Container auf Docker Swarm
  ‚Ä¢ USB Storage (ext4, 1-2TB) auf beiden Nodes
  ‚Ä¢ Multi-User mit Permissions
  ‚Ä¢ GlusterFS Admin Share
  ‚Ä¢ VIP Integration (192.168.1.122)
  ‚Ä¢ Automatischer Sync (st√ºndlich, VIP-aware)
  ‚Ä¢ keepalived mit nopreempt (stabile Failover)

Vorteile vs Host-Based:
  ‚úÖ 80% weniger Code
  ‚úÖ Container Orchestration
  ‚úÖ Einfache Updates
  ‚úÖ Portable Configuration
  ‚úÖ Auto-Restart bei Fehler
  ‚úÖ Swarm-managed Failover
  ‚úÖ VIP-aware Sync (kein Split-Brain)
  ‚úÖ Stable Failover (nopreempt)

Performance:
  ‚Ä¢ USB 3.0 SSD: 100-400 MB/s
  ‚Ä¢ GlusterFS: 100-110 MB/s
  ‚Ä¢ Network: 1 GBit (110 MB/s max)
  ‚Ä¢ Sync: St√ºndlich, nur vom Master

High Availability:
  ‚Ä¢ Failover: Automatisch bei Node-Ausfall
  ‚Ä¢ Failback: Bei totalem Ausfall (nicht bei Recovery)
  ‚Ä¢ nopreempt: Verhindert unn√∂tige Switches
  ‚Ä¢ Sync folgt VIP: Immer Master ‚Üí Backup
  ‚Ä¢ Kein Datenverlust m√∂glich

Access:
  Windows: \\192.168.1.122\<share>
  Linux:   mount -t cifs //192.168.1.122/<share>
  macOS:   smb://192.168.1.122/<share>
```

### N√§chste m√∂gliche Schritte

Du hast jetzt ein produktionsreifes Setup! Optional:

1. **Monitoring**: Grafana Dashboard f√ºr Storage/Performance
2. **Backup zu External**: Automatisches Backup zu USB HDD
3. **Media Server**: Plex/Jellyfin Container Stack
4. **Photo Management**: Immich/PhotoPrism f√ºr Familien-Fotos
5. **Nextcloud**: Alternative zu Samba mit Web-UI
6. **Notifications**: Alerting bei Sync-Fehlern (Email/Telegram)
7. **Performance Monitoring**: iostat, iotop f√ºr USB Performance
8. **Log Aggregation**: Loki + Grafana f√ºr zentrales Logging

---

## Quick Reference

### Wichtige Pfade

```
/mnt/usb-nas-master/data/          # USB Storage (pi-3)
/mnt/usb-nas-backup/data/          # USB Storage (pi-4)
/mnt/glusterfs-swarm/              # GlusterFS Share
/var/lib/docker/volumes/           # Docker Volumes
```

### Wichtige Commands

```bash
# Service Management
docker service ls
docker service ps nas_samba
docker service logs nas_samba -f
docker service update --force nas_samba

# Stack Management
docker stack deploy -c samba-nas-stack.yml nas
docker stack ps nas
docker stack rm nas

# Storage
df -h /mnt/usb-nas-master
df -h /mnt/glusterfs-swarm
du -sh /mnt/usb-nas-master/data/*

# Sync Management
nas-sync-smart.sh                    # Manueller Sync
nas-sync-status.sh                   # Sync Status
tail -f /var/log/nas-sync.log       # Live Logs
crontab -l | grep nas-sync          # Cron Status

# HA Status
check-vip-container.sh              # VIP & Container Check
ip addr show eth0 | grep 192.168.1.122  # VIP Status
docker service ps nas_samba         # Container Location

# keepalived
sudo systemctl status keepalived
sudo journalctl -u keepalived -f
```

### Wichtige IPs

```
192.168.1.101  zolo-pi-1 (Swarm Manager)
192.168.1.102  zolo-pi-2
192.168.1.103  zolo-pi-3 (NAS Master)
192.168.1.104  zolo-pi-4 (NAS Backup)

192.168.1.121  VIP GlusterFS/NFS
192.168.1.122  VIP Samba
```

---

**Setup Time:** ~30 Minuten
**Complexity:** Low
**Maintenance:** Minimal
**Stability:** High

üéâ **Happy Sharing!**